# PushT Human - äº¤äº’å¼æ¨æ–¹å—ç¯å¢ƒ

ä¸€ä¸ªåŸºäºgym-pushtçš„äº¤äº’å¼å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œæ”¯æŒäººç±»é”®ç›˜æ§åˆ¶ã€ç­–ç•¥åˆ‡æ¢ã€è½¨è¿¹è®°å½•å’Œæ•°æ®ä¸Šä¼ åŠŸèƒ½ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹ç‚¹

- **ğŸ® äººæœºäº¤äº’**: æ”¯æŒé”®ç›˜å’Œé¼ æ ‡æ§åˆ¶æ™ºèƒ½ä½“ç§»åŠ¨
- **ğŸ”„ ç­–ç•¥åˆ‡æ¢**: ä¸€é”®åˆ‡æ¢ç”¨æˆ·æ§åˆ¶å’ŒAIç­–ç•¥æ§åˆ¶
- **ğŸ“Š è½¨è¿¹è®°å½•**: è‡ªåŠ¨è®°å½•çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ç­‰å®Œæ•´è½¨è¿¹æ•°æ®
- **ğŸ¬ è½¨è¿¹å›æ”¾**: æ”¯æŒå›æ”¾å·²è®°å½•çš„è½¨è¿¹ï¼ŒåŒ…å«åˆå§‹çŠ¶æ€ä¿¡æ¯
- **ğŸ¤– å¼ºåŒ–å­¦ä¹ è®­ç»ƒ**: åŸºäºstable_baselines3çš„PPOå’ŒSACç®—æ³•è®­ç»ƒ
- **ğŸ“ˆ å®éªŒç®¡ç†**: é›†æˆWandBè¿›è¡Œå®éªŒè·Ÿè¸ªå’Œå¯è§†åŒ–
- **â˜ï¸ æ•°æ®ä¸Šä¼ **: æ”¯æŒå°†æ•°æ®ä¸Šä¼ åˆ°Hugging Face Hub
- **âš™ï¸ é…ç½®ç®¡ç†**: åŸºäºHydraçš„çµæ´»é…ç½®ç³»ç»Ÿ
- **ğŸ¯ å¤šè§‚æµ‹æ¨¡å¼**: æ”¯æŒçŠ¶æ€å‘é‡ã€åƒç´ å›¾åƒç­‰å¤šç§è§‚æµ‹ç±»å‹

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install pygame gymnasium hydra-core omegaconf
pip install datasets huggingface_hub
pip install numpy opencv-python

# å®‰è£…å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¾èµ–
pip install stable-baselines3[extra] wandb torch

# å®‰è£…gym-pushtç¯å¢ƒ (å¯ç¼–è¾‘æ¨¡å¼)
cd gym-pusht
pip install -e .
```

## ğŸ® ä½¿ç”¨æ–¹æ³•

### åŸºç¡€è¿è¡Œ

```bash
python pusht_human.py
```

### å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

#### PPOè®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒPPO
python RL/train_ppo.py

# è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
python RL/train_ppo.py training.total_timesteps=500000
python RL/train_ppo.py ppo.learning_rate=1e-4
python RL/train_ppo.py env.n_envs=8

# ç¦ç”¨WandBæ—¥å¿—
python RL/train_ppo.py wandb.enabled=false

# ä¿®æ”¹æ¨¡å‹ä¿å­˜è·¯å¾„
python RL/train_ppo.py save.model_dir=models/ppo_experiment1
```

#### SACè®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒSAC
python RL/train_sac.py

# è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
python RL/train_sac.py training.total_timesteps=500000
python RL/train_sac.py sac.learning_rate=1e-4
python RL/train_sac.py sac.buffer_size=200000

# è°ƒæ•´æ¢ç´¢å‚æ•°
python RL/train_sac.py sac.ent_coef=0.1

# ä¿®æ”¹å®éªŒåç§°
python RL/train_sac.py experiment.name=SAC_PushT_LargeBuffer
```

#### WandBé…ç½®

```bash
# é¦–æ¬¡ä½¿ç”¨éœ€è¦ç™»å½•WandB
wandb login

# è®¾ç½®é¡¹ç›®åç§°
python RL/train_ppo.py experiment.project=my-pusht-experiments

# ç¦»çº¿æ¨¡å¼è¿è¡Œ
python RL/train_ppo.py wandb.mode=offline
```

### è‡ªå®šä¹‰é…ç½®è¿è¡Œ

```bash
# ä¿®æ”¹æ¸¸æˆè½®æ•°
python pusht_human.py data.num_episodes=10

# ä¿®æ”¹è§‚æµ‹ç±»å‹
python pusht_human.py env.obs_type=pixels

# ä¿®æ”¹å¸§ç‡
python pusht_human.py control.fps=15

# å¯ç”¨è‡ªåŠ¨ä¸Šä¼ 
python pusht_human.py upload.auto_upload=true upload.repo_id=your-username/pusht-demo
```

### ä»…ä¸Šä¼ å·²æœ‰æ•°æ®

```bash
python pusht_human.py upload_only=true upload.repo_id=your-username/pusht-demo
```

### è½¨è¿¹å›æ”¾

```bash
# å›æ”¾æ‰€æœ‰è½¨è¿¹ï¼ˆè‡ªåŠ¨æ’­æ”¾ï¼‰
python replay_trajectories.py --data_path=data/pusht_trajectories/trajectories_5episodes.pkl

# å›æ”¾æŒ‡å®šè½¨è¿¹
python replay_trajectories.py --data_path=data/pusht_trajectories/trajectories_5episodes.pkl --episode_id=0

# æ‰‹åŠ¨é€æ­¥å›æ”¾
python replay_trajectories.py --data_path=data/pusht_trajectories/trajectories_5episodes.pkl --manual_play

# è°ƒæ•´å›æ”¾é€Ÿåº¦
python replay_trajectories.py --data_path=data/pusht_trajectories/trajectories_5episodes.pkl --delay=0.05

# è°ƒæ•´è½¨è¿¹é—´é—´éš”ï¼ˆé»˜è®¤2ç§’ï¼‰
python replay_trajectories.py --data_path=data/pusht_trajectories/trajectories_5episodes.pkl --inter_episode_delay=1

# è¿ç»­å›æ”¾æ— é—´éš”
python replay_trajectories.py --data_path=data/pusht_trajectories/trajectories_5episodes.pkl --inter_episode_delay=0

# æ”¯æŒJSONæ ¼å¼
python replay_trajectories.py --data_path=data/pusht_trajectories/trajectories_5episodes.json
```

#### å›æ”¾æ§åˆ¶è¯´æ˜

**è‡ªåŠ¨æ’­æ”¾æ¨¡å¼**:
- è½¨è¿¹å°†è‡ªåŠ¨æŒ‰ç…§åŸå§‹åŠ¨ä½œåºåˆ—æ‰§è¡Œ
- å¤šä¸ªè½¨è¿¹ä¼šè‡ªåŠ¨è¿ç»­æ’­æ”¾ï¼Œé»˜è®¤è½¨è¿¹é—´é—´éš”2ç§’
- æŒ‰ `Q` é”®å¯éšæ—¶é€€å‡ºå›æ”¾

**æ‰‹åŠ¨æ’­æ”¾æ¨¡å¼**:
- æŒ‰ `ç©ºæ ¼` é”®æ‰§è¡Œä¸‹ä¸€æ­¥
- æŒ‰ `Q` é”®é€€å‡ºå›æ”¾
- å¯ä»¥ä»”ç»†è§‚å¯Ÿæ¯ä¸€æ­¥çš„æ‰§è¡Œç»“æœ

**å›æ”¾å‚æ•°**:
- `--delay`: æ§åˆ¶è‡ªåŠ¨æ’­æ”¾æ—¶æ¯æ­¥ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´
- `--inter_episode_delay`: æ§åˆ¶è½¨è¿¹é—´çš„é—´éš”æ—¶é—´ï¼ˆè®¾ä¸º0å¯è¿ç»­æ’­æ”¾ï¼‰
- `--episode_id`: æŒ‡å®šå›æ”¾ç‰¹å®šè½¨è¿¹çš„ID

## âŒ¨ï¸ æ§åˆ¶è¯´æ˜

| æŒ‰é”® | åŠŸèƒ½ |
|------|------|
| `W` | å‘ä¸Šç§»åŠ¨æ™ºèƒ½ä½“ |
| `S` | å‘ä¸‹ç§»åŠ¨æ™ºèƒ½ä½“ |
| `A` | å‘å·¦ç§»åŠ¨æ™ºèƒ½ä½“ |
| `D` | å‘å³ç§»åŠ¨æ™ºèƒ½ä½“ |
| `ç©ºæ ¼` | åˆ‡æ¢ç”¨æˆ·æ§åˆ¶/ç­–ç•¥æ§åˆ¶æ¨¡å¼ |
| `R` | é‡ç½®å½“å‰ç¯å¢ƒ |
| `Q` | é€€å‡ºæ¸¸æˆ |

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ pusht_human.py              # ä¸»ç¨‹åºæ–‡ä»¶
â”œâ”€â”€ replay_trajectories.py      # è½¨è¿¹å›æ”¾è„šæœ¬
â”œâ”€â”€ utils.py                    # å·¥å…·å‡½æ•°æ¨¡å—
â”œâ”€â”€ RL/                         # å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ train_ppo.py           # PPOè®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ train_sac.py           # SACè®­ç»ƒè„šæœ¬
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ pusht_human.yaml       # ä¸»ç¨‹åºé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ pusht_human_mouse.yaml # é¼ æ ‡æ§åˆ¶é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ replay.yaml            # å›æ”¾é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ rl/                    # å¼ºåŒ–å­¦ä¹ é…ç½®ç›®å½•
â”‚       â”œâ”€â”€ ppo.yaml          # PPOè®­ç»ƒé…ç½®
â”‚       â””â”€â”€ sac.yaml          # SACè®­ç»ƒé…ç½®
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pusht_trajectories/    # è½¨è¿¹æ•°æ®ä¿å­˜ç›®å½•
â”œâ”€â”€ models/                    # è®­ç»ƒæ¨¡å‹ä¿å­˜ç›®å½•
â”‚   â”œâ”€â”€ ppo/                  # PPOæ¨¡å‹
â”‚   â””â”€â”€ sac/                  # SACæ¨¡å‹
â”œâ”€â”€ logs/                      # è®­ç»ƒæ—¥å¿—ç›®å½•
â”‚   â”œâ”€â”€ ppo/                  # PPOæ—¥å¿—
â”‚   â””â”€â”€ sac/                  # SACæ—¥å¿—
â”œâ”€â”€ gym-pusht/                 # PushTç¯å¢ƒæºç 
â””â”€â”€ README.md                  # è¯´æ˜æ–‡æ¡£
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒé…ç½® (`env`)
- `obs_type`: è§‚æµ‹ç±»å‹ï¼Œå¯é€‰ `state`ã€`pixels`ã€`environment_state_agent_pos`ã€`pixels_agent_pos`
- `max_episode_steps`: æ¯è½®æœ€å¤§æ­¥æ•° (é»˜è®¤300)
- `success_threshold`: æˆåŠŸè¦†ç›–ç‡é˜ˆå€¼ (é»˜è®¤0.95)

### æ§åˆ¶é…ç½® (`control`)
- `fps`: æ¸²æŸ“å¸§ç‡ (é»˜è®¤10)
- `keyboard_move_speed`: é”®ç›˜ç§»åŠ¨é€Ÿåº¦ (é»˜è®¤10)
- `user_control`: åˆå§‹æ˜¯å¦ç”¨æˆ·æ§åˆ¶ (é»˜è®¤true)
- `keys`: é”®ç›˜æ˜ å°„é…ç½®

### æ•°æ®é…ç½® (`data`)
- `num_episodes`: æ¸¸æˆè½®æ•° (é»˜è®¤5)
- `save_dir`: æ•°æ®ä¿å­˜ç›®å½• (é»˜è®¤"data/pusht_trajectories")
- `save_format`: ä¿å­˜æ ¼å¼ï¼Œå¯é€‰ `pickle`ã€`json`ã€`npz` (é»˜è®¤pickle)
- `dataset_name`: æ•°æ®é›†åç§° (é»˜è®¤"pusht_human_demo")

### ç­–ç•¥é…ç½® (`policy`)
- `type`: ç­–ç•¥ç±»å‹ï¼Œå½“å‰æ”¯æŒ `random` (é»˜è®¤random)
- `random_seed`: éšæœºç§å­ (é»˜è®¤42)

### ä¸Šä¼ é…ç½® (`upload`)
- `hf_token`: Hugging Face token (ä»ç¯å¢ƒå˜é‡è·å–)
- `repo_id`: ä»“åº“ID (é»˜è®¤"pusht-human-demo")
- `private`: æ˜¯å¦ç§æœ‰ (é»˜è®¤false)
- `auto_upload`: æ¸¸æˆç»“æŸåè‡ªåŠ¨ä¸Šä¼  (é»˜è®¤false)

### å¼ºåŒ–å­¦ä¹ é…ç½® (`RL`)

#### PPOé…ç½® (`configs/rl/ppo.yaml`)
- **å®éªŒé…ç½®**:
  - `experiment.name`: å®éªŒåç§°
  - `experiment.project`: WandBé¡¹ç›®åç§°
  - `experiment.tags`: å®éªŒæ ‡ç­¾
- **ç¯å¢ƒé…ç½®**:
  - `env.obs_type`: è§‚æµ‹ç±»å‹ (æ¨è`pixels_agent_pos`)
  - `env.n_envs`: å¹¶è¡Œç¯å¢ƒæ•°é‡ (é»˜è®¤4)
- **PPOç®—æ³•å‚æ•°**:
  - `ppo.learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤3e-4)
  - `ppo.n_steps`: æ¯æ¬¡æ›´æ–°æ”¶é›†æ­¥æ•° (é»˜è®¤2048)
  - `ppo.batch_size`: æ‰¹æ¬¡å¤§å° (é»˜è®¤64)
  - `ppo.gamma`: æŠ˜æ‰£å› å­ (é»˜è®¤0.99)
- **è®­ç»ƒé…ç½®**:
  - `training.total_timesteps`: æ€»è®­ç»ƒæ­¥æ•° (é»˜è®¤200000)
  - `training.eval_freq`: è¯„ä¼°é¢‘ç‡ (é»˜è®¤10000)
- **WandBé…ç½®**:
  - `wandb.enabled`: æ˜¯å¦å¯ç”¨WandB (é»˜è®¤true)
  - `wandb.mode`: è¿è¡Œæ¨¡å¼ (online/offline/disabled)

#### SACé…ç½® (`configs/rl/sac.yaml`)
- **SACç®—æ³•å‚æ•°**:
  - `sac.learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤3e-4)
  - `sac.buffer_size`: ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å° (é»˜è®¤100000)
  - `sac.learning_starts`: å¼€å§‹å­¦ä¹ çš„æ­¥æ•° (é»˜è®¤10000)
  - `sac.ent_coef`: ç†µç³»æ•° (é»˜è®¤auto)
  - `sac.tau`: è½¯æ›´æ–°å‚æ•° (é»˜è®¤0.005)

## ğŸ“Š æ•°æ®æ ¼å¼

### è½¨è¿¹æ•°æ®ç»“æ„

```python
@dataclass
class TrajectoryStep:
    observation: Any          # è§‚æµ‹æ•°æ®
    action: np.ndarray       # åŠ¨ä½œå‘é‡ [x, y]
    reward: float           # å¥–åŠ±å€¼
    terminated: bool        # æ˜¯å¦ç»ˆæ­¢
    truncated: bool         # æ˜¯å¦æˆªæ–­
    info: Dict[str, Any]    # é¢å¤–ä¿¡æ¯

@dataclass 
class Episode:
    steps: List[TrajectoryStep]  # æ‰€æœ‰æ­¥éª¤
    episode_id: int             # è½®æ¬¡ID
    total_reward: float         # æ€»å¥–åŠ±
    success: bool              # æ˜¯å¦æˆåŠŸ
    length: int                # æ­¥æ•°
    initial_state: Dict[str, Any]  # åˆå§‹çŠ¶æ€ä¿¡æ¯ï¼ˆç”¨äºå›æ”¾ï¼‰
    # åŒ…å«: agent_pos, block_pos, block_angle, goal_pose
```

### ä¿å­˜æ ¼å¼

- **Pickle**: å®Œæ•´çš„Pythonå¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰æ•°æ®ç±»å‹
- **JSON**: æ–‡æœ¬æ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹å’Œå¤„ç†
- **NPZ**: NumPyå‹ç¼©æ ¼å¼ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®åˆ†æ

## ğŸ¤— Hugging Faceé›†æˆ

### è®¾ç½®Token

```bash
# æ–¹æ³•1: ç¯å¢ƒå˜é‡
export HF_TOKEN=your_huggingface_token

# æ–¹æ³•2: é…ç½®æ–‡ä»¶
python pusht_human.py upload.hf_token=your_huggingface_token
```

### æ•°æ®é›†æ ¼å¼

ä¸Šä¼ åˆ°Hugging Faceçš„æ•°æ®é›†åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

- `episode_id`: è½®æ¬¡ç¼–å·
- `step_id`: æ­¥éª¤ç¼–å·  
- `observation`: è§‚æµ‹æ•°æ®
- `action`: åŠ¨ä½œæ•°æ®
- `reward`: å¥–åŠ±å€¼
- `terminated`: ç»ˆæ­¢æ ‡å¿—
- `truncated`: æˆªæ–­æ ‡å¿—
- `episode_success`: è½®æ¬¡æˆåŠŸæ ‡å¿—
- `episode_length`: è½®æ¬¡é•¿åº¦
- `episode_total_reward`: è½®æ¬¡æ€»å¥–åŠ±

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. å¼ºåŒ–å­¦ä¹ ç ”ç©¶
- æ”¶é›†äººç±»ä¸“å®¶æ¼”ç¤ºæ•°æ®
- è¿›è¡Œæ¨¡ä»¿å­¦ä¹ (Imitation Learning)ç ”ç©¶
- è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“(PPO/SAC)
- å¯¹æ¯”äººç±»å’ŒAIç­–ç•¥çš„è¡Œä¸ºå·®å¼‚

### 2. ç®—æ³•åŸºå‡†æµ‹è¯•
- åœ¨PushTç¯å¢ƒä¸Šè¯„ä¼°ä¸åŒRLç®—æ³•æ€§èƒ½
- å¯¹æ¯”åƒç´ è§‚æµ‹å’ŒçŠ¶æ€è§‚æµ‹çš„æ•ˆæœ
- ç ”ç©¶å¤šæ¨¡æ€è§‚æµ‹(åƒç´ +ä½ç½®)çš„ä¼˜åŠ¿

### 3. æ•°æ®é›†æ„å»º
- åˆ›å»ºé«˜è´¨é‡çš„äººç±»æ¼”ç¤ºæ•°æ®é›†
- ä¸ºè¡Œä¸ºå…‹éš†(Behavioral Cloning)æä¾›è®­ç»ƒæ•°æ®
- æ„å»ºåŸºå‡†æµ‹è¯•æ•°æ®é›†

### 4. æ•™å­¦æ¼”ç¤º
- ç›´è§‚å±•ç¤ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
- è®©å­¦ç”Ÿä½“éªŒæ™ºèƒ½ä½“å†³ç­–è¿‡ç¨‹
- å¯¹æ¯”ä¸åŒç­–ç•¥çš„æ•ˆæœ
- å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹å’Œæ€§èƒ½æ›²çº¿

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¼˜åŒ–
1. **è§‚æµ‹ç±»å‹é€‰æ‹©**: 
   - `pixels_agent_pos`: æœ€ä½³æ€§èƒ½ï¼Œç»“åˆè§†è§‰å’Œä½ç½®ä¿¡æ¯
   - `pixels`: çº¯è§†è§‰å­¦ä¹ ï¼Œæ›´å…·æŒ‘æˆ˜æ€§
   - `state`: æœ€å¿«è®­ç»ƒé€Ÿåº¦
2. **å¹¶è¡Œç¯å¢ƒ**: PPOä½¿ç”¨å¤šä¸ªå¹¶è¡Œç¯å¢ƒåŠ é€Ÿæ•°æ®æ”¶é›†
3. **è¶…å‚æ•°è°ƒä¼˜**: 
   - å­¦ä¹ ç‡: 3e-4æ˜¯è‰¯å¥½çš„èµ·ç‚¹
   - PPO n_steps: æ ¹æ®ä»»åŠ¡é•¿åº¦è°ƒæ•´
   - SAC buffer_size: æ›´å¤§çš„ç¼“å†²åŒºé€šå¸¸æ•ˆæœæ›´å¥½
4. **WandBç›‘æ§**: å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦å’Œè¶…å‚æ•°æ•ˆæœ

### æé«˜æ•°æ®è´¨é‡
1. **è°ƒæ•´ç§»åŠ¨é€Ÿåº¦**: æ ¹æ®éœ€è¦è°ƒæ•´`keyboard_move_speed`
2. **é€‚å½“å¸§ç‡**: å¹³è¡¡æµç•…åº¦å’Œæ§åˆ¶ç²¾åº¦
3. **å¤šè½®æ¸¸æˆ**: æ”¶é›†è¶³å¤Ÿçš„æ•°æ®æ ·æœ¬

## ğŸ› å¸¸è§é—®é¢˜

### Q: å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ˜¾å­˜ä¸è¶³ï¼Ÿ
A: é™ä½å¹¶è¡Œç¯å¢ƒæ•°é‡ (`env.n_envs`)ï¼Œæˆ–ä½¿ç”¨è¾ƒå°çš„ç½‘ç»œæ¶æ„ã€‚

### Q: WandBæ— æ³•è¿æ¥ï¼Ÿ
A: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–ä½¿ç”¨ `wandb.mode=offline` è¿›è¡Œç¦»çº¿è®­ç»ƒã€‚

### Q: è®­ç»ƒæ”¶æ•›æ…¢ï¼Ÿ
A: å°è¯•è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¢åŠ ç½‘ç»œå®¹é‡ï¼Œæˆ–ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚

### Q: SACè®­ç»ƒä¸ç¨³å®šï¼Ÿ
A: æ£€æŸ¥ç†µç³»æ•°è®¾ç½®ï¼Œç¡®ä¿ç»éªŒå›æ”¾ç¼“å†²åŒºè¶³å¤Ÿå¤§ã€‚

### Q: é”®ç›˜æ§åˆ¶ä¸å“åº”ï¼Ÿ
A: ç¡®ä¿pygameçª—å£å¤„äºç„¦ç‚¹çŠ¶æ€ï¼Œç‚¹å‡»çª—å£åå†æ“ä½œã€‚

### Q: æ•°æ®ä¸Šä¼ å¤±è´¥ï¼Ÿ
A: æ£€æŸ¥Hugging Face tokenæ˜¯å¦æ­£ç¡®è®¾ç½®ï¼Œç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ã€‚

### Q: æ¸¸æˆå¡é¡¿ï¼Ÿ
A: é™ä½å¸§ç‡è®¾ç½®æˆ–ä½¿ç”¨æ›´è½»é‡çš„è§‚æµ‹ç±»å‹ã€‚

### Q: æ‰¾ä¸åˆ°ä¿å­˜çš„æ•°æ®ï¼Ÿ
A: æ£€æŸ¥`data.save_dir`é…ç½®ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨ä¸”æœ‰å†™æƒé™ã€‚

## ğŸ”§ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„RLç®—æ³•

åœ¨`RL/`ç›®å½•ä¸‹åˆ›å»ºæ–°çš„è®­ç»ƒè„šæœ¬ï¼š

```python
# RL/train_new_algorithm.py
from stable_baselines3 import NewAlgorithm

def setup_model(cfg, env):
    model = NewAlgorithm(
        policy=cfg.algorithm.policy,
        env=env,
        # å…¶ä»–ç®—æ³•ç‰¹å®šå‚æ•°
    )
    return model
```

### è‡ªå®šä¹‰ç‰¹å¾æå–å™¨

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ è‡ªå®šä¹‰ç‰¹å¾æå–å™¨
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

class CustomExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=256):
        super().__init__(observation_space, features_dim)
        # å®šä¹‰ç½‘ç»œæ¶æ„
        
    def forward(self, observations):
        # å®ç°å‰å‘ä¼ æ’­
        return features
```

### æ·»åŠ æ–°ç­–ç•¥

åœ¨`utils.py`ä¸­ç»§æ‰¿åŸºç¡€ç­–ç•¥ç±»ï¼š

```python
class YourPolicy:
    def __init__(self, action_space):
        self.action_space = action_space
    
    def get_action(self, observation):
        # å®ç°ä½ çš„ç­–ç•¥é€»è¾‘
        return action
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºgym-pushtç¯å¢ƒæ„å»ºï¼Œéµå¾ªç›¸åº”çš„å¼€æºè®¸å¯è¯ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›æœ¬é¡¹ç›®ï¼

---

**æ³¨æ„**: ä½¿ç”¨å‰è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…gym-pushtç¯å¢ƒï¼Œè¯¦è§é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„å®‰è£…è¯´æ˜ã€‚ 